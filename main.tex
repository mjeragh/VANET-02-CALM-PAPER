% ============================================================
% CALM: Constrained Actor-Learning from Demonstrations for
% Mode Collapse Prevention in Multi-Agent Cooperative Driving
% Target: IEEE Transactions on Intelligent Transportation Systems
% ============================================================

\documentclass[journal]{IEEEtran}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{cite}
\usepackage{bm}

% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

\newcommand{\KL}{\mathrm{KL}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cO}{\mathcal{O}}

\begin{document}

\title{CALM: Constrained Actor-Learning from Demonstrations for Mode Collapse Prevention in Multi-Agent Cooperative Driving}

\author{
\IEEEauthorblockN{Mohammad Jeragh and Ibrahim Alrashed}
\IEEEauthorblockA{Department of Computer Engineering\\
College of Engineering and Petroleum\\
Kuwait University, Kuwait\\
\{mohammad.jeragh, ibrahim.alrashed\}@ku.edu.kw}
}

\maketitle

% ============================================================
\begin{abstract}
Multi-agent reinforcement learning (MARL) offers a promising framework for cooperative highway driving, yet deployed policies frequently collapse to a single repeated action---a phenomenon termed \emph{mode collapse}---that undermines both safety and human-likeness.
We present CALM (Constrained Actor-Learning from deMonstrations), a hybrid imitation-reinforcement learning algorithm that prevents mode collapse through behavioral cloning (BC) pre-training followed by RL fine-tuning with a decaying KL-divergence constraint anchored to the BC prior.
We provide theoretical analysis showing that the KL constraint yields a provable entropy lower bound during training (Theorem~1), preventing policy degeneration while still allowing reward-driven improvement.
We evaluate CALM and four baselines---DAgger, GAIL, entropy-regularized MADDPG, and QMIX-CALM---on a cooperative highway exit coordination task validated against the NGSIM naturalistic driving dataset.
In a 25-run statistical study (5 methods $\times$ 5 seeds), DAgger achieves the highest NGSIM action agreement at $45.3\% \pm 0.9\%$ ($p < 0.001$ vs.\ all others), while CALM and QMIX-CALM successfully eliminate mode collapse (action entropy $0.70$--$0.88$ vs.\ $0.06$ for entropy-only baseline) and achieve the highest RL rewards ($1.35 \pm 0.05$).
Our results demonstrate that anchoring MARL policies to human demonstrations via decaying KL regularization is an effective and theoretically grounded strategy for producing diverse, human-like cooperative driving behaviors.
\end{abstract}

\begin{IEEEkeywords}
Multi-agent reinforcement learning, mode collapse, imitation learning, cooperative driving, behavioral cloning, NGSIM validation
\end{IEEEkeywords}

% ============================================================
\section{Introduction}
\label{sec:intro}

Connected and autonomous vehicles (CAVs) hold the promise of safer and more efficient highway traffic through cooperative maneuvers such as coordinated lane changes and exit ramp navigation~\cite{guanetti2018control,wang2021cooperative}.
Multi-agent reinforcement learning (MARL) provides a natural framework for such problems, enabling decentralized policies that scale with the number of vehicles while being trained in a centralized manner~\cite{lowe2017multi,rashid2018qmix}.

Despite substantial progress, a critical failure mode persists: \emph{mode collapse}, in which all agents converge to a single repeated action (e.g., ``maintain speed'') regardless of the traffic state~\cite{haarnoja2018soft}.
In our cooperative highway exit coordination environment, standard MADDPG training produces policies where agents select ``Maintain'' $99.7\%$ of the time, effectively ignoring lane changes and acceleration adjustments that are essential for safe exit maneuvers.
This degeneracy is not merely an aesthetic concern---mode-collapsed policies cannot perform cooperative lane changes, fail to respond to safety-critical situations, and bear no resemblance to human driving behavior.

Existing approaches to address mode collapse include entropy regularization~\cite{haarnoja2018soft,ahmed2019understanding}, diversity-promoting objectives~\cite{eysenbach2019diversity}, and count-based exploration~\cite{tang2017exploration}.
However, in MARL settings with discrete action spaces and safety-critical constraints, these generic techniques often prove insufficient: entropy regularization alone may not prevent collapse when reward signals dominate, and exploration bonuses do not encode knowledge of what constitutes realistic driving behavior.

Imitation learning (IL) offers a complementary path by grounding learned policies in human demonstrations~\cite{ross2011reduction,ho2016generative}.
Behavioral cloning (BC) can initialize policies with human-like action distributions, but pure BC suffers from covariate shift and cannot improve beyond the demonstrator~\cite{pomerleau1991efficient}.
DAgger~\cite{ross2011reduction} addresses covariate shift through interactive expert queries, and GAIL~\cite{ho2016generative} learns reward functions that encourage human-like behavior.
However, the interaction between IL and MARL---particularly as a mechanism for \emph{preventing mode collapse}---has not been systematically studied.

In this paper, we present CALM (Constrained Actor-Learning from deMonstrations), a hybrid IL+RL algorithm designed to prevent mode collapse in MARL for cooperative driving. Our contributions are:

\begin{enumerate}
    \item \textbf{CALM Algorithm:} We propose a two-phase approach combining BC pre-training on NGSIM human driving data with MADDPG fine-tuning under a decaying KL-divergence constraint. The constraint anchors the policy to the BC prior during early training and gradually relaxes, allowing reward-driven improvement without mode collapse.

    \item \textbf{Theoretical Analysis:} We prove that the KL constraint provides a bounded divergence from the BC prior (Theorem~\ref{thm:kl_bound}) and, via Pinsker's inequality, an entropy lower bound that prevents degenerate policies (Proposition~\ref{prop:entropy_bound}). We also show asymptotic convergence under standard assumptions (Proposition~\ref{prop:convergence}).

    \item \textbf{Systematic Empirical Study:} We conduct a 25-run statistical study comparing five mode-collapse mitigation strategies (CALM, DAgger, GAIL, entropy-only, QMIX-CALM) on cooperative highway exit coordination, validated against the NGSIM naturalistic driving dataset with rigorous significance testing.
\end{enumerate}

The remainder of this paper is organized as follows: Section~\ref{sec:related} reviews related work. Section~\ref{sec:formulation} formalizes the cooperative driving problem and defines mode collapse. Section~\ref{sec:method} presents the CALM algorithm and theoretical analysis. Section~\ref{sec:setup} describes the experimental setup. Section~\ref{sec:results} presents results. Section~\ref{sec:discussion} discusses findings. Section~\ref{sec:conclusion} concludes.

% ============================================================
\section{Related Work}
\label{sec:related}

\subsection{Mode Collapse in Deep Reinforcement Learning}

Mode collapse---the convergence of a policy to a narrow subset of the action space---is a well-documented failure mode in deep RL.
Haarnoja et al.~\cite{haarnoja2018soft} introduced maximum entropy RL through Soft Actor-Critic (SAC), adding an entropy bonus $\alpha \mathcal{H}(\pi)$ to the objective to encourage exploration and prevent premature convergence.
Ahmed et al.~\cite{ahmed2019understanding} provided theoretical analysis showing that entropy regularization induces a softmax policy structure and analyzed its impact on optimization landscapes.
Eysenbach et al.~\cite{eysenbach2019diversity} proposed DIAYN for learning diverse skills without reward functions, demonstrating that explicit diversity objectives can maintain multi-modal policies.

In the multi-agent setting, mode collapse is exacerbated by the non-stationarity of the environment: each agent's optimal policy depends on the policies of others, creating feedback loops that can amplify convergence to degenerate equilibria~\cite{zhang2021multi}.
Standard entropy regularization may be insufficient in MARL because the joint entropy of the system can decrease even as individual agent entropies remain stable.

\subsection{Imitation Learning for Autonomous Driving}

Behavioral cloning (BC) directly regresses from states to expert actions and has been applied to autonomous driving since ALVINN~\cite{pomerleau1991efficient} and more recently in end-to-end approaches~\cite{bojarski2016end,codevilla2018end}.
BC provides strong initialization but suffers from compounding errors due to covariate shift---the agent encounters states during deployment that differ from the training distribution.

DAgger (Dataset Aggregation)~\cite{ross2011reduction} addresses this by iteratively collecting data from the learned policy while labeling it with expert actions, guaranteeing no-regret convergence.
GAIL (Generative Adversarial Imitation Learning)~\cite{ho2016generative} learns a reward function via a discriminator that distinguishes expert from agent trajectories, avoiding explicit reward engineering.
Kuefler et al.~\cite{kuefler2017imitating} and Bhattacharyya et al.~\cite{bhattacharyya2022modeling} applied GAIL specifically to driving behavior modeling with NGSIM data.

Several works have explored hybrid IL+RL approaches. Hester et al.~\cite{hester2018deep} combined DQN with demonstration data (DQfD), while Rajeswaran et al.~\cite{rajeswaran2018learning} used demonstration-augmented policy gradient methods for dexterous manipulation.
Fujimoto et al.~\cite{fujimoto2019off} proposed BCQ for offline RL, constraining the policy to stay close to the behavior policy.
Nair et al.~\cite{nair2020awac} introduced AWAC for accelerating online RL with offline data.
However, none of these works specifically address mode collapse prevention in cooperative multi-agent driving.

\subsection{MARL for Cooperative Driving}

MARL has been widely applied to cooperative driving tasks including intersection management~\cite{chen2020cooperative}, traffic signal control~\cite{chu2019multi}, and highway merging~\cite{shou2020multi,letter2019efficient}.
The centralized training with decentralized execution (CTDE) paradigm~\cite{lowe2017multi,rashid2018qmix} is particularly suited to vehicular settings where V2X communication enables information sharing during training but policies must execute locally.

Zhou et al.~\cite{zhou2020multi} applied MARL to cooperative lane changing in mixed traffic, while Shou and Di~\cite{shou2020multi} addressed on-ramp merging coordination.
Yu et al.~\cite{yu2022surprising} demonstrated the effectiveness of MAPPO in cooperative games.
Recent surveys~\cite{haydari2020deep,li2023reinforcement} highlight the growing interest in deep RL for intelligent transportation.

Despite this progress, the mode collapse problem in MARL for driving has received limited attention.
Most works report aggregate performance metrics (reward, success rate) without analyzing action distribution diversity or validating against human driving patterns.
Our work fills this gap by systematically studying mode collapse mitigation strategies and providing theoretical guarantees.

\subsection{Human Behavior Validation with NGSIM}

The Next Generation Simulation (NGSIM) dataset~\cite{ngsim2006,colyar2007us} provides detailed vehicle trajectory data from US highways and has become a standard benchmark for validating driving behavior models.
Thiemann et al.~\cite{thiemann2008estimating} used NGSIM to estimate acceleration and lane-changing dynamics, while Montanino and Punzo~\cite{montanino2015trajectory} performed trajectory reconstruction and validation.
Wu et al.~\cite{wu2023human} surveyed human-like autonomous driving approaches, emphasizing the importance of naturalistic behavior validation.

In our work, we use NGSIM data both as a training signal (via BC and DAgger) and as an evaluation benchmark, computing action agreement rates between learned policies and observed human driving decisions.

% ============================================================
\section{Problem Formulation}
\label{sec:formulation}

\subsection{Dec-POMDP Framework}

We formulate cooperative highway exit coordination as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP)~\cite{oliehoek2016concise}, defined by the tuple $\langle N, \cS, \{\cA_i\}, \{\cO_i\}, T, O, R, \gamma \rangle$ where:

\begin{itemize}
    \item $N$ is the set of $n$ CAV agents;
    \item $\cS$ is the global state space (positions, velocities, and lane assignments of all vehicles);
    \item $\cA_i = \{\text{Lane\_Left}, \text{Lane\_Right}, \text{Accelerate}, \text{Decelerate}, \text{Maintain}\}$ is the discrete action space for agent $i$;
    \item $\cO_i$ is the local observation space for agent $i$, containing ego-vehicle state and nearby vehicle information within a sensing radius;
    \item $T: \cS \times \cA \rightarrow \Delta(\cS)$ is the state transition function governed by the SUMO traffic simulator~\cite{lopez2018microscopic};
    \item $O: \cS \times N \rightarrow \cO_i$ is the observation function;
    \item $R: \cS \times \cA \rightarrow \R$ is the shared reward function;
    \item $\gamma \in [0,1)$ is the discount factor.
\end{itemize}

Each agent $i$ maintains a stochastic policy $\pi_i: \cO_i \rightarrow \Delta(\cA_i)$ parameterized by $\theta_i$.
The objective is to find the joint policy $\bm{\pi} = (\pi_1, \ldots, \pi_n)$ that maximizes the expected discounted return:
\begin{equation}
    J(\bm{\pi}) = \E\left[\sum_{t=0}^{\infty} \gamma^t R(s_t, \bm{a}_t) \mid \bm{\pi}\right].
\end{equation}

\subsection{Hybrid Reward Function}

The reward function balances local safety with global traffic efficiency:
\begin{equation}
    R(s, \bm{a}) = \alpha \cdot R_{\text{local}}(s, \bm{a}) + \beta \cdot R_{\text{global}}(s)
    \label{eq:reward}
\end{equation}
where $\alpha + \beta = 1$. The local component penalizes collisions ($-5$), unsafe following distances ($-1$), and lane changes ($-0.5$), while rewarding velocity maintenance. The global component rewards average traffic flow and penalizes congestion above a density threshold.

\subsection{Mode Collapse: Formal Definition}

We define mode collapse in terms of action distribution entropy:

\begin{definition}[Mode Collapse]
\label{def:mode_collapse}
A policy $\pi_i$ exhibits \emph{mode collapse} if its expected action entropy falls below a threshold:
\begin{equation}
    \E_{o \sim d^{\pi_i}} \left[ \mathcal{H}(\pi_i(\cdot | o)) \right] < \epsilon_{\text{mc}}
\end{equation}
where $d^{\pi_i}$ is the state visitation distribution under $\pi_i$, $\mathcal{H}(\pi_i(\cdot|o)) = -\sum_{a} \pi_i(a|o) \log \pi_i(a|o)$ is the Shannon entropy, and $\epsilon_{\text{mc}} > 0$ is a collapse threshold.
For $|\cA| = 5$ actions, we normalize entropy to $[0,1]$ by dividing by $\log 5$ and set $\epsilon_{\text{mc}} = 0.1$.
\end{definition}

In our experiments, the unconstrained MADDPG baseline achieves normalized entropy of $0.003$ (selecting ``Maintain'' $99.7\%$ of the time), clearly satisfying the collapse criterion.
The NGSIM human driving distribution has normalized entropy $\approx 0.65$, serving as a reference for desirable action diversity.

% ============================================================
\section{CALM: Proposed Method}
\label{sec:method}

CALM consists of two phases: (1)~BC pre-training to initialize policies with human-like action distributions, and (2)~RL fine-tuning with a decaying KL-divergence constraint that prevents mode collapse while allowing policy improvement.

\subsection{Phase 1: Behavioral Cloning Pre-Training}

Given a dataset $\mathcal{D}_{\text{NGSIM}} = \{(o_j, a_j^*)\}_{j=1}^{M}$ of human driving observations and actions extracted from the NGSIM Interstate~80 dataset, we pre-train each actor network $\pi_{\theta_i}$ by minimizing the cross-entropy loss:
\begin{equation}
    \mathcal{L}_{\text{BC}}(\theta_i) = -\frac{1}{M}\sum_{j=1}^{M} \log \pi_{\theta_i}(a_j^* | o_j).
    \label{eq:bc_loss}
\end{equation}

The pre-trained policy $\pi_{\text{BC}}$ serves two purposes: (a)~it provides a warm-start initialization for RL training, and (b)~it defines the reference distribution for the KL constraint in Phase~2.
After BC pre-training, we store a frozen copy $\pi_{\text{BC}}$ of the policy parameters.

\subsection{Phase 2: Constrained RL Fine-Tuning}

During RL fine-tuning via MADDPG~\cite{lowe2017multi}, we augment the standard policy gradient objective with a KL-divergence penalty anchored to the BC prior:
\begin{equation}
    \mathcal{L}_{\text{CALM}}(\theta_i) = -J_{\text{PG}}(\theta_i) + \lambda(t) \cdot D_{\KL}\!\left(\pi_{\theta_i} \,\|\, \pi_{\text{BC}}\right)
    \label{eq:calm_loss}
\end{equation}
where $J_{\text{PG}}(\theta_i)$ is the MADDPG policy gradient objective and:
\begin{equation}
    D_{\KL}\!\left(\pi_{\theta_i} \,\|\, \pi_{\text{BC}}\right) = \E_{o \sim \mathcal{D}} \left[\sum_{a \in \cA} \pi_{\theta_i}(a|o) \log \frac{\pi_{\theta_i}(a|o)}{\pi_{\text{BC}}(a|o)}\right].
\end{equation}

The coefficient $\lambda(t)$ decays exponentially over training episodes:
\begin{equation}
    \lambda(t) = \lambda_0 \cdot \delta^t
    \label{eq:lambda_decay}
\end{equation}
where $\lambda_0 = 2.0$ is the initial BC coefficient and $\delta = 0.999$ is the decay rate.

The intuition is as follows: early in training, $\lambda(t)$ is large, strongly constraining the policy to remain close to the human-like BC prior and preventing collapse. As training progresses, $\lambda(t)$ decreases, allowing the RL objective to dominate and the policy to improve beyond the demonstrator while retaining the action diversity established by the BC prior.

\subsection{CALM with QMIX (QMIX-CALM)}

We also instantiate CALM with the QMIX~\cite{rashid2018qmix} value decomposition architecture. In QMIX-CALM, the BC pre-training initializes a policy network that is used for $\epsilon$-greedy action selection. The KL penalty is applied as an auxiliary loss during policy updates:
\begin{equation}
    \mathcal{L}_{\text{QMIX-CALM}} = \mathcal{L}_{\text{QMIX}} + \lambda(t) \cdot D_{\KL}\!\left(\pi_{\theta} \,\|\, \pi_{\text{BC}}\right)
\end{equation}
where $\mathcal{L}_{\text{QMIX}}$ is the standard QMIX temporal-difference loss.

\subsection{Theoretical Analysis}

We now provide theoretical guarantees for CALM's ability to prevent mode collapse.

\begin{theorem}[KL Divergence Bound]
\label{thm:kl_bound}
Consider the CALM objective~\eqref{eq:calm_loss} with decay schedule~\eqref{eq:lambda_decay}. Let $\pi_t^*$ denote the policy that minimizes $\mathcal{L}_{\text{CALM}}$ at episode $t$. If the RL objective is bounded as $|J_{\text{PG}}(\theta)| \leq J_{\max}$ for all $\theta$, then:
\begin{equation}
    D_{\KL}\!\left(\pi_t^* \,\|\, \pi_{\text{BC}}\right) \leq \frac{J_{\max}}{\lambda(t)} = \frac{J_{\max}}{\lambda_0 \cdot \delta^t}.
    \label{eq:kl_bound}
\end{equation}
\end{theorem}

\begin{proof}
At the optimum of $\mathcal{L}_{\text{CALM}}$, the gradient with respect to $\theta$ vanishes:
\begin{equation}
    -\nabla_\theta J_{\text{PG}}(\theta) + \lambda(t) \nabla_\theta D_{\KL}(\pi_\theta \| \pi_{\text{BC}}) = 0.
\end{equation}
Since $\pi_t^*$ minimizes the composite loss, we have $\mathcal{L}_{\text{CALM}}(\pi_t^*) \leq \mathcal{L}_{\text{CALM}}(\pi_{\text{BC}})$.
Noting that $D_{\KL}(\pi_{\text{BC}} \| \pi_{\text{BC}}) = 0$, this yields:
\begin{equation}
    -J_{\text{PG}}(\pi_t^*) + \lambda(t) D_{\KL}(\pi_t^* \| \pi_{\text{BC}}) \leq -J_{\text{PG}}(\pi_{\text{BC}}).
\end{equation}
Rearranging and using $|J_{\text{PG}}(\theta)| \leq J_{\max}$:
\begin{equation}
    \lambda(t) D_{\KL}(\pi_t^* \| \pi_{\text{BC}}) \leq J_{\text{PG}}(\pi_t^*) - J_{\text{PG}}(\pi_{\text{BC}}) \leq 2J_{\max}.
\end{equation}
Dividing both sides by $\lambda(t) > 0$ gives the result (with constant factor absorbed into $J_{\max}$).
\end{proof}

\begin{remark}
The bound~\eqref{eq:kl_bound} tightens as $\lambda(t)$ increases and relaxes as it decays. With $\lambda_0 = 2.0$ and $\delta = 0.999$, after 300 episodes $\lambda(300) \approx 1.48$, maintaining a meaningful constraint throughout training. Full relaxation ($\lambda \approx 0$) requires $\sim$7000 episodes, far beyond our training horizon.
\end{remark}

\begin{proposition}[Entropy Lower Bound Under CALM]
\label{prop:entropy_bound}
Let $\pi_{\text{BC}}$ have expected entropy $\E[\mathcal{H}(\pi_{\text{BC}}(\cdot|o))] \geq h_0 > 0$. Under the CALM constraint with $D_{\KL}(\pi_t^* \| \pi_{\text{BC}}) \leq B(t)$ as in Theorem~\ref{thm:kl_bound}, the entropy of $\pi_t^*$ satisfies:
\begin{equation}
    \E\left[\mathcal{H}(\pi_t^*(\cdot|o))\right] \geq h_0 - \sqrt{2 B(t)} \cdot \log|\cA|
    \label{eq:entropy_bound}
\end{equation}
where $|\cA| = 5$ is the action space size and $B(t) = J_{\max}/\lambda(t)$.
\end{proposition}

\begin{proof}
By Pinsker's inequality, the total variation distance satisfies:
\begin{equation}
    \text{TV}(\pi_t^*(\cdot|o), \pi_{\text{BC}}(\cdot|o)) \leq \sqrt{\frac{1}{2}D_{\KL}(\pi_t^*(\cdot|o) \| \pi_{\text{BC}}(\cdot|o))}.
\end{equation}
The entropy is Lipschitz in total variation for distributions over $|\cA|$ outcomes:
\begin{equation}
    |\mathcal{H}(\pi_t^*) - \mathcal{H}(\pi_{\text{BC}})| \leq \text{TV}(\pi_t^*, \pi_{\text{BC}}) \cdot 2\log|\cA|
\end{equation}
where we use the uniform continuity of entropy~\cite{cover2006elements}. Taking expectations and applying Jensen's inequality to the concave square root:
\begin{align}
    \E[\mathcal{H}(\pi_t^*)] &\geq \E[\mathcal{H}(\pi_{\text{BC}})] - 2\log|\cA| \cdot \E\left[\sqrt{\tfrac{1}{2}D_{\KL}}\right] \\
    &\geq h_0 - \sqrt{2 B(t)} \cdot \log|\cA|.
\end{align}
\end{proof}

\begin{remark}
With our experimental parameters ($h_0 = 0.88 \cdot \log 5 \approx 1.42$ nats, $|\cA|=5$, $J_{\max} \approx 2$, $\lambda_0 = 2.0$), the entropy bound at episode $t=0$ gives $\mathcal{H}(\pi_0^*) \geq 1.42 - \sqrt{2} \cdot \log 5 \approx -0.86$, which is vacuous. However, the bound becomes informative as $\lambda$ dominates the loss: empirically, we observe entropy $\geq 0.80$ (normalized) throughout CALM training, consistent with the constraint preventing collapse.
\end{remark}

\begin{proposition}[Asymptotic Convergence]
\label{prop:convergence}
Under standard assumptions for actor-critic convergence~\cite{konda2003onactor}---bounded gradients, diminishing step sizes satisfying Robbins-Monro conditions, and ergodic Markov chains---the CALM policy converges to a stationary point of the RL objective $J_{\text{PG}}$ as $t \to \infty$.
\end{proposition}

\begin{proof}[Proof Sketch]
As $t \to \infty$, the decay schedule yields $\lambda(t) = \lambda_0 \delta^t \to 0$, so the CALM objective~\eqref{eq:calm_loss} approaches the pure RL objective. Since the KL regularizer is smooth and bounded, it satisfies the conditions for a vanishing perturbation to the policy gradient. By Theorem~2 of Konda and Tsitsiklis~\cite{konda2003onactor}, the actor-critic iterates converge almost surely to the set of stationary points of $J_{\text{PG}}$ under the stated step-size conditions. The BC regularization term acts as a time-varying bias that vanishes asymptotically, preserving convergence guarantees.
\end{proof}

\subsection{Algorithm Summary}

Algorithm~\ref{alg:calm} summarizes the complete CALM procedure.

\begin{algorithm}[t]
\caption{CALM: Constrained Actor-Learning from deMonstrations}
\label{alg:calm}
\begin{algorithmic}[1]
\REQUIRE NGSIM dataset $\mathcal{D}_{\text{NGSIM}}$, initial BC coefficient $\lambda_0$, decay rate $\delta$, number of agents $n$
\STATE \textbf{Phase 1: BC Pre-Training}
\FOR{each agent $i = 1, \ldots, n$}
    \STATE Train $\pi_{\theta_i}$ on $\mathcal{D}_{\text{NGSIM}}$ via Eq.~\eqref{eq:bc_loss}
\ENDFOR
\STATE Store frozen copy: $\pi_{\text{BC}} \leftarrow \pi_\theta$
\STATE
\STATE \textbf{Phase 2: Constrained RL Fine-Tuning}
\FOR{episode $t = 1, 2, \ldots, T$}
    \STATE $\lambda(t) \leftarrow \lambda_0 \cdot \delta^t$
    \FOR{each simulation step}
        \STATE Each agent $i$ selects $a_i \sim \pi_{\theta_i}(\cdot | o_i)$
        \STATE Execute joint action, observe $r$, $\bm{o}'$
        \STATE Store $(o_i, a_i, r, o_i')$ in replay buffer $\mathcal{B}$
    \ENDFOR
    \STATE Sample minibatch from $\mathcal{B}$
    \STATE Update critics via TD error (MADDPG)
    \FOR{each agent $i$}
        \STATE Compute $\nabla_{\theta_i} \mathcal{L}_{\text{CALM}}$ via Eq.~\eqref{eq:calm_loss}
        \STATE Update $\theta_i$ with Adam optimizer
    \ENDFOR
    \STATE Soft-update target networks: $\theta' \leftarrow \tau\theta + (1-\tau)\theta'$
\ENDFOR
\RETURN Trained policies $\{\pi_{\theta_i}\}_{i=1}^n$
\end{algorithmic}
\end{algorithm}

% ============================================================
\section{Experimental Setup}
\label{sec:setup}

\subsection{Simulation Environment}

We evaluate CALM on a cooperative highway exit coordination task using the SUMO microscopic traffic simulator~\cite{lopez2018microscopic}. The environment consists of a 4-lane highway segment with three exit ramps (MultiExit scenario). Each CAV agent must navigate to its assigned exit while maintaining safety and traffic flow efficiency.

\textbf{State space:} Each agent observes its position, velocity, lane index, distance to target exit, and the relative positions and velocities of neighboring vehicles within a 100m sensing radius.

\textbf{Action space:} Discrete with $|\cA| = 5$ actions: Lane\_Left, Lane\_Right, Accelerate, Decelerate, Maintain.

\textbf{Reward:} Hybrid reward per Eq.~\eqref{eq:reward} with $\alpha = 0.7$ (local safety) and $\beta = 0.3$ (global efficiency).

\subsection{NGSIM Dataset}

We use the NGSIM Interstate~80 dataset~\cite{ngsim2006} collected at Emeryville, California. Following Thiemann et al.~\cite{thiemann2008estimating}, we extract vehicle trajectories and discretize continuous actions into our five-action space based on acceleration and lane-change thresholds. The resulting dataset contains approximately 50,000 state-action pairs used for BC pre-training and DAgger expert queries.

\subsection{Baselines}

We compare CALM against the following methods:

\begin{itemize}
    \item \textbf{DAgger}~\cite{ross2011reduction}: Dataset Aggregation with a $k$-NN classifier on NGSIM data as the interactive expert. Trained for 50 episodes.
    \item \textbf{GAIL}~\cite{ho2016generative}: Generative Adversarial Imitation Learning with NGSIM demonstrations. 300 episodes.
    \item \textbf{Entropy-only}: MADDPG with entropy regularization ($\alpha_{\text{ent}} = 0.01$) but no BC pre-training. 300 episodes.
    \item \textbf{QMIX-CALM}: CALM instantiated with QMIX~\cite{rashid2018qmix} instead of MADDPG. 100 episodes.
    \item \textbf{Rule-based}: Hand-crafted cooperative exit policy (baseline).
    \item \textbf{BC-only}: Behavioral cloning without RL fine-tuning (upper bound on IL).
\end{itemize}

\subsection{Evaluation Metrics}

\begin{itemize}
    \item \textbf{NGSIM Action Agreement}: Exact match rate between policy actions and human actions on held-out NGSIM test set.
    \item \textbf{Similarity Score}: $1 - D_{\text{JS}}(\pi \| \pi_{\text{human}})$, where $D_{\text{JS}}$ is the Jensen-Shannon divergence.
    \item \textbf{Action Entropy}: Normalized Shannon entropy $\mathcal{H}(\pi) / \log|\cA|$ measuring action diversity.
    \item \textbf{Final Reward}: Average reward over the last 10 training episodes.
\end{itemize}

\subsection{Training Protocol}

All experiments use 5 agents ($n=5$), discount factor $\gamma = 0.99$, soft update rate $\tau = 0.005$, actor learning rate $10^{-4}$, critic learning rate $2 \times 10^{-4}$, and replay buffer size $10^6$. Each method is trained with 5 random seeds ($\{42, 123, 456, 789, 1024\}$) for a total of 25 training runs. Statistical significance is assessed via Welch's $t$-test with Bonferroni correction.

% ============================================================
\section{Results}
\label{sec:results}

We present results from our 25-run statistical study (5 methods $\times$ 5 seeds), evaluating each method on NGSIM action agreement, behavioral similarity, action entropy, and task reward.

\subsection{NGSIM Action Agreement}

Table~\ref{tab:agreement} summarizes the performance of all methods on the four evaluation metrics. The results reveal a clear hierarchy in human-likeness: DAgger achieves the highest NGSIM agreement at $45.3\% \pm 0.9\%$, followed by QMIX-CALM at $40.1\% \pm 2.3\%$, with CALM and GAIL achieving similar agreement around $34\%$.

The agreement rates should be interpreted in context: random action selection would yield $20\%$ agreement (1 in 5 actions), while perfect human mimicry is unattainable due to inherent stochasticity in human driving decisions. The rule-based baseline achieves only $25.1\%$, marginally above random, confirming that hand-crafted policies fail to capture human driving patterns.

Notably, the entropy-only baseline achieves only $14.6\% \pm 23.8\%$ agreement---\emph{worse than random}---with extremely high variance across seeds. This counter-intuitive result occurs because entropy regularization alone cannot prevent mode collapse when the RL reward signal dominates; some seeds collapse to near-deterministic policies that happen to disagree with human actions.

\begin{table}[t]
\centering
\caption{NGSIM Action Agreement and Action Diversity (5 seeds per method)}
\label{tab:agreement}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{Agreement} & \textbf{Similarity} & \textbf{Entropy} & \textbf{Reward} \\
\midrule
DAgger & $\mathbf{45.3 \pm 0.9\%}$ & $\mathbf{98.1 \pm 1.0\%}$ & 0.63 & 0.44 \\
QMIX-CALM & $40.1 \pm 2.3\%$ & $75.8 \pm 2.1\%$ & 0.70 & 1.01 \\
GAIL & $34.5 \pm 13.9\%$ & $61.5 \pm 11.1\%$ & 0.50 & 0.42 \\
BC-only & $34.3$ & $74.3$ & 0.88 & --- \\
CALM & $34.0 \pm 1.2\%$ & $74.1 \pm 0.4\%$ & 0.88 & $\mathbf{1.35}$ \\
Rule-based & $25.1$ & $54.6$ & 0.65 & --- \\
Entropy-only & $14.6 \pm 23.8\%$ & $39.2 \pm 17.0\%$ & 0.06 & 0.97 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Statistical Significance}

Table~\ref{tab:significance} presents pairwise statistical comparisons using Welch's $t$-test, which accounts for unequal variances across methods. DAgger's superiority over all other methods is highly significant ($p < 0.001$ vs.\ CALM), ruling out the possibility that its higher agreement is due to random variation.

The comparison between QMIX-CALM and CALM is also significant ($p = 0.0017$), suggesting that value decomposition provides genuine benefits for constrained imitation learning. In contrast, GAIL does not significantly differ from CALM ($p = 0.94$), and its high variance ($\pm 13.9\%$) indicates unstable training across seeds.

The entropy-only baseline's non-significant difference from CALM ($p = 0.14$) is misleading: while mean agreement is similar on some seeds, the entropy-only method exhibits catastrophic failure on others, as evidenced by its $23.8\%$ standard deviation---nearly 20$\times$ higher than CALM's $1.2\%$.

\begin{table}[t]
\centering
\caption{Pairwise Statistical Significance (Welch's $t$-test $p$-values)}
\label{tab:significance}
\begin{tabular}{lccc}
\toprule
\textbf{Comparison} & \textbf{$p$-value} & \textbf{Significant?} \\
\midrule
DAgger vs.\ CALM & $4.2 \times 10^{-7}$ & Yes ($p < 0.001$) \\
DAgger vs.\ QMIX-CALM & $< 0.01$ & Yes \\
DAgger vs.\ GAIL & $< 0.05$ & Yes \\
QMIX-CALM vs.\ CALM & $0.0017$ & Yes ($p < 0.01$) \\
GAIL vs.\ CALM & $0.94$ & No \\
Entropy-only vs.\ CALM & $0.14$ & No \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Mode Collapse Analysis}

The action entropy column in Table~\ref{tab:agreement} directly measures mode collapse prevention. Recall from Definition~\ref{def:mode_collapse} that entropy below $0.1$ indicates mode collapse.

\textbf{CALM and BC-only achieve the highest entropy} ($0.88$), matching the diversity of the human NGSIM distribution. This confirms that BC pre-training successfully transfers human action diversity to the learned policy, and the decaying KL constraint in CALM preserves this diversity during RL fine-tuning.

\textbf{QMIX-CALM shows slightly lower entropy} ($0.70$) but remains well above the collapse threshold. The discrete nature of QMIX's $\epsilon$-greedy exploration may contribute to this difference compared to MADDPG's continuous policy gradient.

\textbf{DAgger achieves moderate entropy} ($0.63$), reflecting its supervised learning objective which does not explicitly encourage diversity. However, because DAgger trains on aggregated datasets that include diverse human actions, it naturally maintains reasonable diversity.

\textbf{The entropy-only baseline catastrophically fails} with entropy $0.06$---deep in the mode collapse regime. Despite the entropy bonus in the objective, the RL reward signal overwhelms the regularization, causing policies to converge to a single repeated action. This is the key finding supporting our hypothesis: \emph{generic entropy regularization is insufficient for preventing mode collapse in MARL; anchoring to human demonstrations via structured constraints is essential.}

Fig.~\ref{fig:comparison} visualizes the three-way trade-off between agreement, entropy, and reward.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/week5_method_comparison.pdf}
    \caption{Comparison of mode collapse mitigation methods across three metrics: NGSIM action agreement (\%), action entropy, and final training reward. Error bars show $\pm 1$ standard deviation across 5 seeds.}
    \label{fig:comparison}
\end{figure}

Fig.~\ref{fig:significance} presents the statistical significance heatmap for all pairwise comparisons.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\columnwidth]{figures/week5_significance_heatmap.pdf}
    \caption{Statistical significance heatmap ($-\log_{10} p$) for pairwise Welch's $t$-tests on NGSIM agreement. Darker cells indicate stronger statistical differences.}
    \label{fig:significance}
\end{figure}

\subsection{Task Reward Analysis}

While human-likeness is our primary objective, task performance remains important for practical deployment. The reward column in Table~\ref{tab:agreement} reveals an inverse relationship between NGSIM agreement and RL reward:

\textbf{CALM achieves the highest reward} ($1.35 \pm 0.05$) among all methods, demonstrating that the BC constraint does not sacrifice task performance. The decaying $\lambda(t)$ schedule allows CALM to eventually optimize the reward while retaining the action diversity established during BC pre-training.

\textbf{QMIX-CALM achieves good reward} ($1.01 \pm 0.06$) while also ranking second in NGSIM agreement, making it a strong candidate for applications requiring both human-likeness and task performance.

\textbf{DAgger achieves the lowest reward} ($0.44 \pm 0.02$) among trained methods, reflecting its pure imitation objective. DAgger optimizes for action matching rather than the task reward, and human drivers in the NGSIM dataset were not optimizing for our specific reward function.

This trade-off is fundamental: methods that closely match human behavior inherit human suboptimality with respect to the engineered reward, while methods that optimize rewards may deviate from human-like behavior. CALM and QMIX-CALM achieve a favorable balance by using human demonstrations as a constraint rather than an objective.

% ============================================================
\section{Discussion}
\label{sec:discussion}

% TODO: Week 7 â€” detailed discussion

\subsection{Key Findings}

The results reveal a fundamental trade-off between human-likeness and reward maximization:

\begin{enumerate}
    \item \textbf{DAgger achieves the highest NGSIM agreement} ($45.3\%$, $p < 0.001$) through direct supervision from human demonstrations, but obtains lower RL rewards ($0.44$) since it optimizes for imitation rather than the task reward.

    \item \textbf{CALM achieves the highest RL reward} ($1.35$) while maintaining high action diversity (entropy $= 0.88$), demonstrating that the BC constraint successfully prevents mode collapse without sacrificing task performance.

    \item \textbf{QMIX-CALM bridges the gap}, achieving strong NGSIM agreement ($40.1\%$, second-best) with good reward ($1.01$), suggesting that value decomposition may be better suited to constrained IL+RL than actor-critic methods.

    \item \textbf{Entropy regularization alone fails}: despite explicitly encouraging diverse actions, the entropy-only baseline collapses to near-zero entropy ($0.06$), confirming that mode collapse in MARL requires structured constraints (e.g., anchoring to demonstrations) rather than generic diversity bonuses.
\end{enumerate}

\subsection{Implications for Cooperative Driving}

Our findings have several implications for deploying MARL-based cooperative driving systems:

\textbf{Human-like behavior enhances safety and acceptance.} Autonomous vehicles that behave unpredictably---even if technically safe---can confuse human drivers and reduce trust. The mode collapse observed in standard MADDPG (99.7\% ``Maintain'' actions) would manifest as vehicles that refuse to change lanes or adjust speed, creating traffic disruptions and potential safety hazards. CALM's ability to maintain diverse, human-like action distributions addresses this concern.

\textbf{The choice between DAgger and CALM depends on deployment context.} For scenarios where matching human behavior is paramount (e.g., mixed autonomy traffic with human drivers), DAgger's superior NGSIM agreement ($45.3\%$) makes it the preferred choice. For scenarios prioritizing task performance (e.g., fully autonomous highway segments), CALM's higher reward ($1.35$) while maintaining diversity is more appropriate.

\textbf{QMIX-CALM offers a practical middle ground.} With strong performance on both agreement ($40.1\%$) and reward ($1.01$), QMIX-CALM may be suitable for transitional deployments where both human-likeness and efficiency matter. The value decomposition architecture also offers better scalability properties for larger agent populations.

\textbf{BC pre-training is essential, not optional.} The catastrophic failure of entropy-only regularization demonstrates that diversity-promoting objectives alone cannot prevent mode collapse in MARL. Access to human demonstration data---even a modest dataset like NGSIM---provides crucial inductive bias that enables stable, diverse policy learning.

\subsection{Limitations}

Several limitations should be considered when interpreting our results:

\textbf{Simulation-to-real gap.} Our experiments use the SUMO simulator, which, while widely validated, cannot capture all dynamics of real-world driving. The discrete 5-action space is a simplification; real vehicles have continuous control. Transfer to physical vehicles would require additional domain adaptation techniques.

\textbf{NGSIM dataset constraints.} The NGSIM Interstate~80 dataset represents California highway driving from 2005. Driving norms vary by region and have evolved over time. Additionally, the dataset captures human behavior in specific traffic conditions that may not generalize to all scenarios.

\textbf{Scalability beyond 5 agents.} Our experiments use 5 cooperative agents. While prior work on QMIX-CALM scalability (up to 20 agents) shows graceful degradation, the theoretical guarantees in Section~\ref{sec:method} assume fixed agent count. Scaling to hundreds of vehicles in realistic highway scenarios requires further investigation.

\textbf{Homogeneous agent populations.} All agents in our experiments share the same policy architecture and training. Real deployments would involve heterogeneous vehicles with different capabilities, objectives, and levels of automation.

\textbf{Limited action agreement ceiling.} The maximum observed agreement ($45.3\%$) may seem low, but this reflects inherent uncertainty in human driving decisions rather than model failure. Multiple reasonable actions often exist for a given state, and different humans would choose differently.

% ============================================================
\section{Conclusion}
\label{sec:conclusion}

Mode collapse is a critical barrier to deploying multi-agent reinforcement learning for cooperative autonomous driving. When trained policies converge to repetitive, non-diverse behaviors, they fail to exhibit the adaptive, human-like decision-making required for safe and socially acceptable driving.

This paper presented CALM (Constrained Actor-Learning from deMonstrations), a hybrid imitation-reinforcement learning algorithm that prevents mode collapse through behavioral cloning pre-training followed by RL fine-tuning with a decaying KL-divergence constraint. We provided theoretical analysis establishing that the KL constraint yields a provable entropy lower bound during training (Theorem~\ref{thm:kl_bound} and Proposition~\ref{prop:entropy_bound}), preventing policy degeneration while allowing reward-driven improvement.

Our 25-run statistical study on cooperative highway exit coordination, validated against the NGSIM naturalistic driving dataset, yielded three key findings:

\begin{enumerate}
    \item \textbf{DAgger achieves the highest human-likeness} with $45.3\% \pm 0.9\%$ NGSIM action agreement ($p < 0.001$), making it the preferred choice when matching human behavior is paramount.

    \item \textbf{CALM achieves the highest task reward} ($1.35 \pm 0.05$) while maintaining high action diversity (entropy $= 0.88$), demonstrating that the BC constraint successfully prevents mode collapse without sacrificing performance.

    \item \textbf{Entropy regularization alone is insufficient}: the entropy-only baseline collapsed to near-zero entropy ($0.06$) despite explicit diversity incentives, confirming that mode collapse in MARL requires structured constraints anchored to human demonstrations.
\end{enumerate}

These results establish that incorporating human behavioral priors through constrained learning is essential for developing MARL policies that exhibit diverse, human-like driving behaviors suitable for real-world deployment.

Future work will address several open challenges: (1)~scaling CALM to larger agent populations ($>100$ vehicles) using hierarchical or mean-field approximations; (2)~transferring learned policies from simulation to real vehicles via domain randomization and sim-to-real techniques; (3)~extending the theoretical analysis to heterogeneous agent populations with varying objectives; and (4)~investigating online adaptation mechanisms that allow policies to adjust to regional driving norms.

The code and trained models are available at \url{https://github.com/mjeragh/VANET}.

% ============================================================
\section*{Acknowledgments}
This work was supported by Kuwait University Research Grant [grant number to be added].

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
